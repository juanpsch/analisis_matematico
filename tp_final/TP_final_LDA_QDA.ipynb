{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpJ7s_SIVu_I"
      },
      "source": [
        "# Trabajo Práctico Final: Linear/Quadratic Discriminant Analysis (LDA/QDA)\n",
        "\n",
        "### Definición: Clasificador Bayesiano\n",
        "\n",
        "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
        "\n",
        "De esta manera dicha probabilidad *a posteriori* resulta\n",
        "$$\n",
        "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
        "$$\n",
        "\n",
        "La regla de decisión de Bayes es entonces\n",
        "$$\n",
        "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
        "$$\n",
        "\n",
        "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
        "\n",
        "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
        "\n",
        "### Distribución condicional\n",
        "\n",
        "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
        "\n",
        "Por definición, se tiene entonces que para una clase $j$:\n",
        "$$\n",
        "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
        "$$\n",
        "\n",
        "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
        "\n",
        "### LDA\n",
        "\n",
        "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "### Entrenamiento/Ajuste\n",
        "\n",
        "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
        "\n",
        "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
        "\n",
        "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
        "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
        "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
        "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
        "\n",
        "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
        "\n",
        "### Predicción\n",
        "\n",
        "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TDWOgpJWKQa"
      },
      "source": [
        "## Estructura del código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yEV8WbiWl6k"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "teF9O9JJmG7Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import det, inv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sDBLvbTtlwzs"
      },
      "outputs": [],
      "source": [
        "class ClassEncoder:\n",
        "  def fit(self, y):\n",
        "    self.names = np.unique(y)\n",
        "    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n",
        "    self.fmt = y.dtype\n",
        "    # Q1: por que no hace falta definir un class_to_name para el mapeo inverso?\n",
        "\n",
        "  def _map_reshape(self, f, arr):\n",
        "    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
        "    # Q2: por que hace falta un reshape?\n",
        "\n",
        "  def transform(self, y):\n",
        "    return self._map_reshape(lambda name: self.name_to_class[name], y)\n",
        "\n",
        "  def fit_transform(self, y):\n",
        "    self.fit(y)\n",
        "    return self.transform(y)\n",
        "\n",
        "  def detransform(self, y_hat):\n",
        "    return self._map_reshape(lambda idx: self.names[idx], y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m0KYC8_uSOu4"
      },
      "outputs": [],
      "source": [
        "class BaseBayesianClassifier:\n",
        "  def __init__(self):\n",
        "    self.encoder = ClassEncoder()\n",
        "\n",
        "  def _estimate_a_priori(self, y):\n",
        "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
        "    # Q3: para que sirve bincount?\n",
        "    return np.log(a_priori)\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate all needed parameters for given model\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def fit(self, X, y, a_priori=None):\n",
        "    # first encode the classes\n",
        "    y = self.encoder.fit_transform(y)\n",
        "\n",
        "    # if it's needed, estimate a priori probabilities\n",
        "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
        "\n",
        "    # check that a_priori has the correct number of classes\n",
        "    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
        "\n",
        "    # now that everything else is in place, estimate all needed parameters for given model\n",
        "    self._fit_params(X, y)\n",
        "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n",
        "\n",
        "  def predict(self, X):\n",
        "    # this is actually an individual prediction encased in a for-loop\n",
        "    m_obs = X.shape[1]\n",
        "    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
        "\n",
        "    for i in range(m_obs):\n",
        "      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
        "      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
        "\n",
        "    # return prediction as a row vector (matching y)\n",
        "    return y_hat.reshape(1,-1)\n",
        "\n",
        "  def _predict_one(self, x):\n",
        "    # calculate all log posteriori probabilities (actually, +C)\n",
        "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
        "                  in enumerate(self.log_a_priori) ]\n",
        "\n",
        "    # return the class that has maximum a posteriori probability\n",
        "    return np.argmax(log_posteriori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IRamFdiGDuSR"
      },
      "outputs": [],
      "source": [
        "class QDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate each covariance matrix\n",
        "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
        "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "    # Q7: que hace axis=1? por que no axis=0?\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_covs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fRtC9HEkO5Hu"
      },
      "outputs": [],
      "source": [
        "class TensorizedQDA(QDA):\n",
        "\n",
        "    def _fit_params(self, X, y):\n",
        "        # ask plain QDA to fit params\n",
        "        super()._fit_params(X,y)\n",
        "\n",
        "        # stack onto new dimension\n",
        "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
        "        self.tensor_means = np.stack(self.means)\n",
        "\n",
        "    def _predict_log_conditionals(self,x):\n",
        "        unbiased_x = x - self.tensor_means\n",
        "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
        "\n",
        "        return 0.5*np.log(det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        # return the class that has maximum a posteriori probability\n",
        "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS_zoK-gWkRf"
      },
      "source": [
        "## Código para pruebas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz19b6NJed2A"
      },
      "source": [
        "Seteamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m05KrhUDINVs"
      },
      "outputs": [],
      "source": [
        "# hiperparámetros\n",
        "rng_seed = 6543"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hkXcoldXOqs",
        "outputId": "2ce8d627-3433-4bdd-d370-85f6b703a7b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (150, 4), Y:(150, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris, fetch_openml\n",
        "\n",
        "def get_iris_dataset():\n",
        "  data = load_iris()\n",
        "  X_full = data.data\n",
        "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "  return X_full, y_full\n",
        "\n",
        "def get_penguins():\n",
        "    # get data\n",
        "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True)\n",
        "\n",
        "    # drop non-numeric columns\n",
        "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
        "\n",
        "    # drop rows with missing values\n",
        "    mask = df.isna().sum(axis=1) == 0\n",
        "    df = df[mask]\n",
        "    tgt = tgt[mask]\n",
        "\n",
        "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
        "\n",
        "# showing for iris\n",
        "X_full, y_full = get_iris_dataset()\n",
        "\n",
        "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAk-UQCjKecT",
        "outputId": "9566d67a-b78b-4809-bb94-8f605b065db6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# peek data matrix\n",
        "X_full[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdzMURX2KVdO",
        "outputId": "af5fc3ac-b391-4769-de47-44cea4f566c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa']], dtype='<U10')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# peek target vector\n",
        "y_full[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl8UFh1OegbJ"
      },
      "source": [
        "Separamos el dataset en train y test para medir performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKP_QmWCIECs",
        "outputId": "07798c6a-aa54-430e-d46d-becc2a4315ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 90) (1, 90) (4, 60) (1, 60)\n"
          ]
        }
      ],
      "source": [
        "# preparing data, train - test validation\n",
        "# 70-30 split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_transpose(X, y, test_sz, random_state):\n",
        "    # split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.4, random_state=rng_seed)\n",
        "\n",
        "    # transpose so observations are column vectors\n",
        "    return X_train.T, y_train.T, X_test.T, y_test.T\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  return (y_true == y_pred).mean()\n",
        "\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n",
        "\n",
        "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwgXFPbJemb_"
      },
      "source": [
        "Entrenamos un QDA y medimos su accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dGIf2TA5SpoT"
      },
      "outputs": [],
      "source": [
        "qda = QDA()\n",
        "\n",
        "qda.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0Q30DyLWpTL",
        "outputId": "dbccae86-840c-412f-ed97-22cfac21238a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train (apparent) error is 0.0111 while test error is 0.0167\n"
          ]
        }
      ],
      "source": [
        "train_acc = accuracy(train_y, qda.predict(train_x))\n",
        "test_acc = accuracy(test_y, qda.predict(test_x))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QcLtNNIevC_"
      },
      "source": [
        "Con el magic %%timeit podemos estimar el tiempo que tarda en correr una celda en base a varias ejecuciones. Por poner un ejemplo, acá vamos a estimar lo que tarda un ciclo completo de QDA y también su inferencia (predicción).\n",
        "\n",
        "Ojo! a veces [puede ser necesario ejecutarlo varias veces](https://stackoverflow.com/questions/10994405/python-timeit-results-cached-instead-of-calculated) para obtener resultados consistentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnZT-HN2fUuW",
        "outputId": "2618e7c1-7a77-4285-bafb-c2880ad167a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.52 ms ± 12.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "\n",
        "qda.predict(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjFbVSqfeHUX",
        "outputId": "0254a727-a1d5-4be3-b73a-2f55d2c84a25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.68 ms ± 14.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "\n",
        "model = QDA()\n",
        "model.fit(train_x, train_y)\n",
        "model.predict(test_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Consigna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yb1V7_yXRfO"
      },
      "source": [
        "\n",
        "\n",
        "## Implementación\n",
        "1. Entrenar un modelo QDA sobre el dataset *iris* utilizando las distribuciones *a priori* a continuación ¿Se observan diferencias?¿Por qué cree? _Pista: comparar con las distribuciones del dataset completo, **sin splitear**_.\n",
        "    1. Uniforme (cada clase tiene probabilidad 1/3)\n",
        "    2. Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)\n",
        "2. Repetir el punto anterior para el dataset *penguin*.\n",
        "3. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n",
        "4. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Las conclusiones previas se mantienen?\n",
        "5. Estimar y comparar los tiempos de predicción de las clases `QDA` y `TensorizedQDA`. De haber diferencias ¿Cuáles pueden ser las causas?\n",
        "\n",
        "\n",
        "**Sugerencia:** puede resultar de utilidad para los puntos de comparación utilizar tablas del siguiente estilo:\n",
        "\n",
        "<center>\n",
        "\n",
        "Modelo | Dataset | Seed | Error (train) | Error (test)\n",
        ":---: | :---: | :---: | :---: | :---:\n",
        "QDA | Iris | 125 | 0.55 | 0.85\n",
        "LDA | Iris | 125 | 0.22 | 0.8\n",
        "\n",
        "</center>\n",
        "\n",
        "## Preguntas teóricas\n",
        "\n",
        "1. En LDA se menciona que la función a maximizar puede ser, mediante operaciones, convertida en:\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "Mostrar los pasos por los cuales se llega a dicha expresión.\n",
        "2. Explicar, utilizando las respectivas funciones a maximizar, por qué QDA y LDA son \"quadratic\" y \"linear\".\n",
        "3. La implementación de QDA estima la probabilidad condicional utilizando `0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x` que no es *exactamente* lo descrito en el apartado teórico ¿Cuáles son las diferencias y por qué son expresiones equivalentes?\n",
        "\n",
        "El espíritu de esta componente práctica es la de establecer un mínimo de trabajo aceptable para su entrega; se invita al alumno a explorar otros aspectos que generen curiosidad, sin sentirse de ninguna manera limitado por la consigna.\n",
        "\n",
        "## Ejercicio teórico\n",
        "\n",
        "Sea una red neuronal de dos capas, la primera de 3 neuronas y la segunda de 1 con los parámetros inicializados con los siguientes valores:\n",
        "$$\n",
        "w^{(1)} =\n",
        "\\begin{pmatrix}\n",
        "0.1 & -0.5 \\\\\n",
        "-0.3 & -0.9 \\\\\n",
        "0.8 & 0.02\n",
        "\\end{pmatrix},\n",
        "b^{(1)} = \\begin{pmatrix}\n",
        "0.1 \\\\\n",
        "0.5 \\\\\n",
        "0.8\n",
        "\\end{pmatrix},\n",
        "w^{(2)} =\n",
        "\\begin{pmatrix}\n",
        "-0.4 & 0.2 & -0.5\n",
        "\\end{pmatrix},\n",
        "b^{(2)} = 0.7\n",
        "$$\n",
        "\n",
        "y donde cada capa calcula su salida vía\n",
        "\n",
        "$$\n",
        "y^{(i)} = \\sigma (w^{(i)} \\cdot x^{(i)}+b^{(i)})\n",
        "$$\n",
        "\n",
        "donde $\\sigma (z) = \\frac{1}{1+e^{-z}}$ es la función sigmoidea .\n",
        "\n",
        "\\\\\n",
        "Dada la observación $x=\\begin{pmatrix}\n",
        "1.8 \\\\\n",
        "-3.4\n",
        "\\end{pmatrix}$, $y=5$ y la función de costo $J(\\theta)=\\frac{1}{2}(\\hat{y}_\\theta-y)^2$, calcular las derivadas de J respecto de cada parámetro $w^{(1)}$, $w^{(2)}$, $b^{(1)}$, $b^{(2)}$.\n",
        "\n",
        "*Nota: Con una sigmoidea a la salida jamás va a poder estimar el 5 \"pedido\", pero eso no afecta al mecanismo de backpropagation!*\n",
        "\n",
        "## Preguntas en el código\n",
        "Previamente las preguntas \"técnicas\" en comentarios en el código eran parte del TP, y buscaban que el alumno logre entrar en el detalle de por qué cada linea de código es como es y en el orden en el que está. Ya no forman parte de la consigna, pero se aconseja al alumno intentar responderlas. Las respuestas a las mismas se encuentran en un archivo separado.\n",
        "\n",
        "## Opcional\n",
        "\n",
        "### QDA\n",
        "\n",
        "Debido a la forma cuadrática de QDA, no se puede predecir para *n* observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de *n x n* en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
        "\n",
        "1. Implementar el modelo `FasterQDA` (se recomienda heredarlo de TensorizedQDA) de manera de eliminar el ciclo for en el método predict.\n",
        "2. Comparar los tiempos de predicción de `FasterQDA` con `TensorizedQDA` y `QDA`\n",
        "3. Mostrar (puede ser con un print) dónde aparece la mencionada matriz de *n x n*, donde *n* es la cantidad de observaciones a predecir.\n",
        "4.Demostrar\n",
        "$$\n",
        "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
        "$$ es decir, que se puede \"esquivar\" la matriz de *n x n* usando matrices de *n x p*.\n",
        "5.Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente. ¿Hay cambios en los tiempos de predicción?\n",
        "\n",
        "\n",
        "### LDA\n",
        "\n",
        "1. \"Tensorizar\" el modelo LDA y comparar sus tiempos de predicción con el modelo antes implementado. *Notar que, en modo tensorizado, se puede directamente precomputar $\\mu^T \\cdot \\Sigma^{-1} \\in \\mathbb{R}^{k \\times 1 \\times p}$ y guardar eso en vez de $\\Sigma^{-1}$.*\n",
        "2. LDA no sufre del problema antes descrito de QDA debido a que no computa productos internos, por lo que no tiene un verdadero costo extra en memoria predecir \"en batch\". Implementar el modelo `FasterLDA` y comparar sus tiempos de predicción con las versiones anteriores de LDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Resolución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementación\n",
        "1. Entrenar un modelo QDA sobre el dataset *iris* utilizando las distribuciones *a priori* a continuación ¿Se observan diferencias?¿Por qué cree? _Pista: comparar con las distribuciones del dataset completo, **sin splitear**_.\n",
        "    1. Uniforme (cada clase tiene probabilidad 1/3)\n",
        "    2. Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)\n",
        "2. Repetir el punto anterior para el dataset *penguin*.\n",
        "3. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n",
        "4. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Las conclusiones previas se mantienen?\n",
        "5. Estimar y comparar los tiempos de predicción de las clases `QDA` y `TensorizedQDA`. De haber diferencias ¿Cuáles pueden ser las causas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_priori = []\n",
        "a_priori.append(None)\n",
        "a_priori.append([1/3,1/3,1/3])\n",
        "a_priori.append([0.9,0.05,0.05])\n",
        "a_priori.append([0.05,0.9,0.05])\n",
        "a_priori.append([0.05,0.05,0.9])\n",
        "\n",
        "def comparar(X, y, a_priori, model):\n",
        "\n",
        "    model_list = []    \n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "\n",
        "    for idx, i in enumerate (a_priori):\n",
        "        \n",
        "        modeli = model()\n",
        "        model_list.append(modeli)\n",
        "        model_list[idx].fit(train_x, train_y, a_priori=i)\n",
        "        train_acc.append(accuracy(train_y, model_list[idx].predict(train_x)))\n",
        "        test_acc.append(accuracy(test_y, model_list[idx].predict(test_x)))\n",
        "        \n",
        "    return model_list, train_acc, test_acc\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iris Dataset\n",
        "X_full, y_full = get_iris_dataset()\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n",
        "\n",
        "model_list1, train_acc1, test_acc1 = comparar(train_x, train_y, a_priori, QDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QDA 0 | a_priori = None | log(a_priori) = [-1.2039728  -1.13251384 -0.97344915]:\n",
            "Dataset: Iris\n",
            "Train (apparent) error is 0.0111 while test error is 0.0167\n",
            "\n",
            "QDA 1 | a_priori = [0.3333333333333333, 0.3333333333333333, 0.3333333333333333] | log(a_priori) = [-1.09861229 -1.09861229 -1.09861229]:\n",
            "Dataset: Iris\n",
            "Train (apparent) error is 0.0222 while test error is 0.0167\n",
            "\n",
            "QDA 2 | a_priori = [0.9, 0.05, 0.05] | log(a_priori) = [-0.10536052 -2.99573227 -2.99573227]:\n",
            "Dataset: Iris\n",
            "Train (apparent) error is 0.0222 while test error is 0.0167\n",
            "\n",
            "QDA 3 | a_priori = [0.05, 0.9, 0.05] | log(a_priori) = [-2.99573227 -0.10536052 -2.99573227]:\n",
            "Dataset: Iris\n",
            "Train (apparent) error is 0.0333 while test error is 0.0000\n",
            "\n",
            "QDA 4 | a_priori = [0.05, 0.05, 0.9] | log(a_priori) = [-2.99573227 -2.99573227 -0.10536052]:\n",
            "Dataset: Iris\n",
            "Train (apparent) error is 0.0333 while test error is 0.0500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for idx, i in enumerate (a_priori):\n",
        "    print(f\"QDA {idx} | a_priori = {i} | log(a_priori) = {model_list1[idx].log_a_priori}:\")\n",
        "    print(f\"Dataset: Iris\")\n",
        "    print(f\"Train (apparent) error is {1-train_acc1[idx]:.4f} while test error is {1-test_acc1[idx]:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se observan diferencias dependiendo de la probabilidad a priori utilizada.  \\\n",
        "En el gráfico de abajo se ve que las clases se distribuyen en partes iguales 1/3 cada una. \\\n",
        "En este caso es evidente que la elección de la probabilidad a priori va a influenciar la performance \\\n",
        "Al pasarle datos de train, la estimación la hace sobre datos de train la cual no está balanceada exáctamente \\\n",
        "como en los datos totales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plt.hist(y_full)\n",
        "plt.hist([train_y.ravel(), y_full.ravel()], label=['Train', 'Full'], density=True)\n",
        "plt.legend()\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\pytorchgpu\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# Penguins Dataset\n",
        "X_full, y_full = get_penguins()\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n",
        "\n",
        "model_list2, train_acc2, test_acc2 = comparar(train_x, train_y, a_priori, QDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QDA 0 | a_priori = None | log(a_priori) = [-0.89219318 -1.65944833 -0.91629073]:\n",
            "Dataset: Penguins\n",
            "Train (apparent) error is 0.0146 while test error is 0.0146\n",
            "\n",
            "QDA 1 | a_priori = [0.3333333333333333, 0.3333333333333333, 0.3333333333333333] | log(a_priori) = [-1.09861229 -1.09861229 -1.09861229]:\n",
            "Dataset: Penguins\n",
            "Train (apparent) error is 0.0098 while test error is 0.0073\n",
            "\n",
            "QDA 2 | a_priori = [0.9, 0.05, 0.05] | log(a_priori) = [-0.10536052 -2.99573227 -2.99573227]:\n",
            "Dataset: Penguins\n",
            "Train (apparent) error is 0.0195 while test error is 0.0219\n",
            "\n",
            "QDA 3 | a_priori = [0.05, 0.9, 0.05] | log(a_priori) = [-2.99573227 -0.10536052 -2.99573227]:\n",
            "Dataset: Penguins\n",
            "Train (apparent) error is 0.0098 while test error is 0.0219\n",
            "\n",
            "QDA 4 | a_priori = [0.05, 0.05, 0.9] | log(a_priori) = [-2.99573227 -2.99573227 -0.10536052]:\n",
            "Dataset: Penguins\n",
            "Train (apparent) error is 0.0098 while test error is 0.0073\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for idx, i in enumerate (a_priori):\n",
        "    print(f\"QDA {idx} | a_priori = {i} | log(a_priori) = {model_list2[idx].log_a_priori}:\")\n",
        "    print(f\"Dataset: Penguins\")\n",
        "    print(f\"Train (apparent) error is {1-train_acc2[idx]:.4f} while test error is {1-test_acc2[idx]:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plt.hist(y_full)\n",
        "plt.hist([train_y.ravel(), y_full.ravel()], label=['Train', 'Full'], density=True)\n",
        "plt.legend()\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate global covariance matrix\n",
        "    self.inv_cov = inv(np.cov(X, bias=True))\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used    \n",
        "    return self.means[class_idx].T @ self.inv_cov @ (x - 0.5*self.means[class_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iris Dataset\n",
        "X_full, y_full = get_iris_dataset()\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n",
        "\n",
        "model_list3, train_acc3, test_acc3 = comparar(train_x, train_y, a_priori, LDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LDA 0 | a_priori = None | log(a_priori) = [-1.2039728  -1.13251384 -0.97344915]:\n",
            "Dataset: Iris\n",
            "Train (apparent) error is 0.1222 while test error is 0.2000\n",
            "\n",
            "LDA 1 | a_priori = [0.3333333333333333, 0.3333333333333333, 0.3333333333333333] | log(a_priori) = [-1.09861229 -1.09861229 -1.09861229]:\n",
            "Dataset: Iris\n",
            "Train (apparent) error is 0.1111 while test error is 0.1833\n",
            "\n",
            "LDA 2 | a_priori = [0.9, 0.05, 0.05] | log(a_priori) = [-0.10536052 -2.99573227 -2.99573227]:\n",
            "Dataset: Iris\n",
            "Train (apparent) error is 0.5111 while test error is 0.5000\n",
            "\n",
            "LDA 3 | a_priori = [0.05, 0.9, 0.05] | log(a_priori) = [-2.99573227 -0.10536052 -2.99573227]:\n",
            "Dataset: Iris\n",
            "Train (apparent) error is 0.5889 while test error is 0.6333\n",
            "\n",
            "LDA 4 | a_priori = [0.05, 0.05, 0.9] | log(a_priori) = [-2.99573227 -2.99573227 -0.10536052]:\n",
            "Dataset: Iris\n",
            "Train (apparent) error is 0.5000 while test error is 0.6000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for idx, i in enumerate (a_priori):\n",
        "    print(f\"LDA {idx} | a_priori = {i} | log(a_priori) = {model_list3[idx].log_a_priori}:\")\n",
        "    print(f\"Dataset: Iris\")\n",
        "    print(f\"Train (apparent) error is {1-train_acc3[idx]:.4f} while test error is {1-test_acc3[idx]:.4f}\\n\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\pytorchgpu\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# Penguins Dataset\n",
        "X_full, y_full = get_penguins()\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n",
        "\n",
        "model_list4, train_acc4, test_acc4 = comparar(train_x, train_y, a_priori, LDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LDA 0 | a_priori = None | log(a_priori) = [-0.89219318 -1.65944833 -0.91629073]:\n",
            "Dataset: Penguins\n",
            "Train (apparent) error is 0.0195 while test error is 0.0219\n",
            "\n",
            "LDA 1 | a_priori = [0.3333333333333333, 0.3333333333333333, 0.3333333333333333] | log(a_priori) = [-1.09861229 -1.09861229 -1.09861229]:\n",
            "Dataset: Penguins\n",
            "Train (apparent) error is 0.0098 while test error is 0.0073\n",
            "\n",
            "LDA 2 | a_priori = [0.9, 0.05, 0.05] | log(a_priori) = [-0.10536052 -2.99573227 -2.99573227]:\n",
            "Dataset: Penguins\n",
            "Train (apparent) error is 0.4683 while test error is 0.3577\n",
            "\n",
            "LDA 3 | a_priori = [0.05, 0.9, 0.05] | log(a_priori) = [-2.99573227 -0.10536052 -2.99573227]:\n",
            "Dataset: Penguins\n",
            "Train (apparent) error is 0.3805 while test error is 0.4161\n",
            "\n",
            "LDA 4 | a_priori = [0.05, 0.05, 0.9] | log(a_priori) = [-2.99573227 -2.99573227 -0.10536052]:\n",
            "Dataset: Penguins\n",
            "Train (apparent) error is 0.4098 while test error is 0.4745\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for idx, i in enumerate (a_priori):\n",
        "    print(f\"LDA {idx} | a_priori = {i} | log(a_priori) = {model_list4[idx].log_a_priori}:\")\n",
        "    print(f\"Dataset: Penguins\")\n",
        "    print(f\"Train (apparent) error is {1-train_acc4[idx]:.4f} while test error is {1-test_acc4[idx]:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se observa que el QDA es superior al LDA, lo cual tiene sentido ya que asumir que las covarianzas son iguales para cada clase es una asumsion muy grande."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\pytorchgpu\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n",
            "d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\pytorchgpu\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "## Seed1\n",
        "rng_seed2 = 1568\n",
        "\n",
        "# Iris Dataset\n",
        "X_full, y_full = get_iris_dataset()\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed2)\n",
        "\n",
        "model_list5, train_acc5, test_acc5 = comparar(train_x, train_y, a_priori, LDA)\n",
        "\n",
        "# Penguins Dataset\n",
        "X_full, y_full = get_penguins()\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed2)\n",
        "\n",
        "model_list6, train_acc6, test_acc6 = comparar(train_x, train_y, a_priori, LDA)\n",
        "\n",
        "## Seed2\n",
        "rng_seed3 = 234\n",
        "\n",
        "# Iris Dataset\n",
        "X_full, y_full = get_iris_dataset()\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed3)\n",
        "\n",
        "model_list7, train_acc7, test_acc7 = comparar(train_x, train_y, a_priori, LDA)\n",
        "\n",
        "# Penguins Dataset\n",
        "X_full, y_full = get_penguins()\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed3)\n",
        "\n",
        "model_list8, train_acc8, test_acc8 = comparar(train_x, train_y, a_priori, LDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo   Dataset    Seed       Error (train)   Error (test)   \n",
            "QDA      Iris       6543       0.0111          0.0167    \n",
            "QDA      Iris       6543       0.0222          0.0167    \n",
            "QDA      Iris       6543       0.0222          0.0167    \n",
            "QDA      Iris       6543       0.0333          0.0000    \n",
            "QDA      Iris       6543       0.0333          0.0500    \n",
            "QDA      Penguins   6543       0.0146          0.0146    \n",
            "QDA      Penguins   6543       0.0098          0.0073    \n",
            "QDA      Penguins   6543       0.0195          0.0219    \n",
            "QDA      Penguins   6543       0.0098          0.0219    \n",
            "QDA      Penguins   6543       0.0098          0.0073    \n",
            "LDA      Iris       6543       0.1222          0.2000    \n",
            "LDA      Iris       6543       0.1111          0.1833    \n",
            "LDA      Iris       6543       0.5111          0.5000    \n",
            "LDA      Iris       6543       0.5889          0.6333    \n",
            "LDA      Iris       6543       0.5000          0.6000    \n",
            "LDA      Penguins   6543       0.0195          0.0219    \n",
            "LDA      Penguins   6543       0.0098          0.0073    \n",
            "LDA      Penguins   6543       0.4683          0.3577    \n",
            "LDA      Penguins   6543       0.3805          0.4161    \n",
            "LDA      Penguins   6543       0.4098          0.4745    \n",
            "QDA      Iris       1568       0.1222          0.2000    \n",
            "QDA      Iris       1568       0.1111          0.1833    \n",
            "QDA      Iris       1568       0.5111          0.5000    \n",
            "QDA      Iris       1568       0.5889          0.6333    \n",
            "QDA      Iris       1568       0.5000          0.6000    \n",
            "QDA      Penguins   1568       0.0195          0.0219    \n",
            "QDA      Penguins   1568       0.0098          0.0073    \n",
            "QDA      Penguins   1568       0.4683          0.3577    \n",
            "QDA      Penguins   1568       0.3805          0.4161    \n",
            "QDA      Penguins   1568       0.4098          0.4745    \n",
            "LDA      Iris       234        0.1222          0.2000    \n",
            "LDA      Iris       234        0.1111          0.1833    \n",
            "LDA      Iris       234        0.5111          0.5000    \n",
            "LDA      Iris       234        0.5889          0.6333    \n",
            "LDA      Iris       234        0.5000          0.6000    \n",
            "LDA      Penguins   234        0.0195          0.0219    \n",
            "LDA      Penguins   234        0.0098          0.0073    \n",
            "LDA      Penguins   234        0.4683          0.3577    \n",
            "LDA      Penguins   234        0.3805          0.4161    \n",
            "LDA      Penguins   234        0.4098          0.4745    \n"
          ]
        }
      ],
      "source": [
        "d = [['QDA', 'Iris',     rng_seed,  train_acc1, test_acc1],\n",
        "     ['QDA', 'Penguins', rng_seed,  train_acc2, test_acc2],\n",
        "     ['LDA', 'Iris',     rng_seed,  train_acc3, test_acc3],\n",
        "     ['LDA', 'Penguins', rng_seed,  train_acc4, test_acc4],\n",
        "     ['QDA', 'Iris',     rng_seed2, train_acc5, test_acc5],\n",
        "     ['QDA', 'Penguins', rng_seed2, train_acc6, test_acc6],\n",
        "     ['LDA', 'Iris',     rng_seed3, train_acc7, test_acc7],\n",
        "     ['LDA', 'Penguins', rng_seed3, train_acc8, test_acc8],\n",
        "    ]\n",
        "    \n",
        "headers = 'Modelo', 'Dataset', 'Seed', 'Error (train)', 'Error (test)'\n",
        "\n",
        "print(\"{:<8} {:<10} {:<10} {:<15} {:<15}\".format('Modelo', 'Dataset', 'Seed', 'Error (train)', 'Error (test)'))\n",
        "\n",
        "for v in d:\n",
        "    Modelo, Dataset, Seed, accu_train, accu_test = v\n",
        "    for idx, i in enumerate (a_priori):\n",
        "        print(\"{:<8} {:<10} {:<10} {:<15} {:<10}\".format(Modelo, Dataset, Seed, f'{1-accu_train[idx]:.4f}', f'{1-accu_test[idx]:.4f}'))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
